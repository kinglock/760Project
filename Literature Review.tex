
\documentclass[9pt, technote]{IEEEtran}

\usepackage{alltt} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{hyperref}

\everymath{\displaystyle}
\hypersetup{pdfborder = {0 0 0}}

\title{Concept Drift Handling of Imbalanced Data Streams: a Literature Review}
\author{R. Anderson, K. Chen and S. Jin}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We review the literature available around classification of imbalanced data in a streaming environment. First, we introduce the particular problems faced within streaming environments and with imbalanced classes. Through examining current research in the field, we explore the facets of classification in this environment: sampling techniques; approaches to detecting concept drift; base learners for building models to classify unseen data; and methods of utilising ensembles to improve classification accuracy. Finally, we examine evaluation of these methods, and the pertinent measures used.
\end{abstract}

\section{Introduction}
In recent year, as computing power and hardware technologies become more powerful and robust, itÕs possible to store and process large volume of data. Such data sets which continuously and rapidly grow over time are referred to as data streams. Mining data streams brings challenges to traditional data mining techniques, as most of the traditional data mining techniques cannot simply apply to data streams due to: 1. Traditional data mining techniques need process the data multiple times, usually works only on static dataset. 2.  Data streamsÕ temporal locality or concept drift nature, which makes it more difficult for traditional data mining techniques to adapt, because the class distribution changes as the data stream evolving over time in the context of classification.
Furthermore, itÕs more challenging to improve the overall accuracy of classification if the classesÕ distribution are imbalanced, which is especially important for many applications in some fields, such as fraud detection and network intrusion detection. 
\\\\ 
This literature survey gives an overall picture on handling concept drift in imbalanced data streams from different angles such as sampling techniques, ensemble techniques, change detection algorithms and adaptive learning algorithms,and evaluation methodologies of adaptive learning algorithms.                                             
The survey is organized as follows. In Section 1, we introduce the problem of concept drift and imbalance problem in data stream. Section 2 presents several sampling methods that are commonly used in data stream mining and imbalanced problem.  Section 3 presents several common drift detection algorithms. Section 4 discusses several popular classification learning algorithms that are commonly used for skewed data streams. Section 5 discusses ensemble techniques. Section 6 discusses the evaluation methodologies that are close to skewed concept-drifted data stream mining area. Section 7 concludes the survey. 
\\\\
When data comes in the form of streams, it is impossible to fit the entire data into machineÕs memory for processing, hence only online processing is feasible for mining data streams. Online processing refers to the predictive models are trained incrementally. However, this only solves the computational issue. In a dynamically changing environment, concept drift refers to the underlying class distribution of data stream can change over time. For example, online shopping customerÕs interest can change over time. To solve the problem of concept drift, predictive models needs to be updated online, which is also called online adaptive learning, this approach explicitly employs extra techniques to handle concept drift without changing the base learning algorithms, however, it is also possible to modify the base learning algorithms to handle concept drift, such as Hellinger Distance Decision Tree(HDDT\cite{hddt}.) There are two strategies for updating the learning model: 1. update the model at regular intervals without considering whether there is a change occurred; 2.  detect a concept change before updating the model. In this survey, we mainly discuss the second approach. Well known change detection algorithms are: Statistical Process Control (SPC\cite{spc}), ADaptative WINDdowing (ADWIN\cite{adwin}) , Fixed Cumulative Windows Model (FCWM\cite{FCWM}), Page Hinkley Test (PHT\cite{PHT}), Early Drift Detection Method(EDDM\cite{eddm}) and FLORA\cite{FLORA}. Detailed review of these algorithms are in in section 4.
\\\\
Imbalanced data set has a skewed class distribution, usually one majority class distribution, one or more minority class distribution. The problem has been well researched since 2000, but more recent researches started applying some approaches to data streams. Three different approaches are: 1. data level approach. This approach mostly applies sampling technique, which simply modifies the data set in order to rebalance the class distribution; 2. algorithm level approach, which adapted to deal with minority class to improve performance; 3. cost sensitive approach has a cost matrix to describe the cost of misclassifying a class instance. Among them, data level approach is mostly used one in the context of data streams. Sample Ensemble Framework is a representative one in this approach. Most of the data level techniques for handling concept drift combined with ensemble techniques to improve accuracy. Data level approach has advantages of not changing the algorithm. However, the data level approach suffers from the problems from resource and computation. On the other hand, algorithm level approach doesnÕt need sampling as a pre-processing step, thus doesnÕt have the problem from resource and computation.


\section{Sampling methods}
There are two categories of sampling techniques that can be used for concept-drifted stream mining, one category aims to keep the training set small for fast processing, representative ones are Sliding Windows and Reservoir Sampling\cite{reservoirsampling}. Another category is to improve performance on the imbalanced data set, SMO-TE\cite{SMOTE}(Synthetic Minority Over-sampling Technique), SERA\cite{SERA}(Selectively Recursive Approach) framework, MuSeRA\cite{musera}(Multiple Selectively Recursive Approach), REA\cite{rea}(Recursive Ensemble Approach) are among this category. Balancing training data is the most straight-forward approach for imbalanced concept-drifting data stream mining.  Many sampling algorithms improved accuracy by under-sampling the majority class or over-sampling the minority class. In "A General Framework for Mining Concept-Drifting Data Streams with Skewed Distributions", Gao proved that the sampling technique can reduce error by collecting positive instances and keeping them in training set. Synthetic Minority Over-sampling Technique (SMOTE), the minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbours, rather than by over-sampling with replacement. SMOTE forces decision region of minority class to become more general. SMOTE gives better classification accuracy in terms of ROC. SERA, MuSeRA, REA took the similar approach either over-sampling, under-sampling or combined both. Neighbourhood Cleaning Rule (NCL\cite{ncl}) uses the WilsonÕs Edited Nearest Neighbor Rule (ENN\cite{enn}) to remove majority class instances. In \cite{classifier_ensemble}, the authors suggested using Classifier Ensemble together with Sampling techniques can improve performance. Sampling techniques is used at data level, which may keep the learning algorithm unchanged, however it could bring computational and memory issues when applying to data stream mining.
\section{Concept drift detection}
Drift detection methods provide feedback to the learner by detecting when concept drifts have occurred. These methods may be independent of the learning model, or structurally embedded inside a classifying algorithm such as RCD (Goncalves \& Barros, 2013). When a concept drift has been detected by the detector, the learner is modified or retrained on some set of instances. This allows the learner to adapt and respond to changes which may lead to a higher classification accuracy compared to blind or non-adaptive approaches. Gama et al suggest one advantage of this approach is that it provides additional information about the way data was generated (Gama, 2014). However this may create more false positives - particularly problematic for noisy data. A variety of change detection methods have been proposed - earlier work focused on sequential analysis techniques, but adaptive windowing and statistical methods are also popular approaches. I will summarise some techniques below and highlight recent work on detection methods that addresses the class imbalance problem. 
\\\\
The CUSUM method is a sequential analysis technique that detects changes based on evaluating the cumulative residue of the learner and testing if there is a significant departure from zero. When the cumulative value is greater than some threshold (lambda) then it indicates a drift has occurred.
The Page-Hinkley test is a variation on the CUSUM method that is used to detect sudden shifts in Gaussian signals. An important application of this method is in signal processing. It records a minimum cumulative value in addition to the current cumulative value. Both The Page-Hinkley and the CUSUM methods only require a small amount of memory to process instances O(1).
\\\\
The drift detection method (DDM) proposed by Gama et al records the current mean error rate $(p_i)$, and error standard deviation $(s_i)$ as well as the minimum values $(p_{min}$, $s_{min})$ as each new instance is seen (Gama et al, 2004). A normal distribution is used to set the thresholds as the number of errors approximates a Bernoulli distribution for samples sizes greater than 30. This means that there is at least a delay of 30 errors until a drift is detected (Wang et al). The warning threshold is defined as $p_{min} + 2 \times s_{min}$ and is updated with the arrival of new instances. A warning is raised when the current value of pi + si is greater or equal to the warning threshold and subsequent instances are stored in a warning window. If the error rate surpasses the drift threshold of $p_{min} + 3 \times s_{min}$, then the learner is retrained on instances stored in the warning window and the minimum values $(p_{min} and s_{min})$ are reset. An analysis done by Goncalves et al on real and synthetic datasets shows that DDM performed most favourably on datasets affected by gradual drifts based on accuracies and the average ranking.
The Early Drift Detection method (EDDM) is a variation on the drift detection method proposed by Gama et al (2004) and was designed to handle gradual concept drifts (Baena-Garcia et al 2006). It differs in that it uses the distance of the errors rather than the error rate as a measure of change. The main strength of this algorithm is the ability to detect slow gradual shifts. On the other hand it may be less resistant to noise, and is much less effective for detecting sudden shifts. Hence it may have a high false alarm and miss detection rate for these datasets (as shown in Goncalves et als? experiments using datasets with abrupt drifts).  Similar to DDM, this method may also have a delay in drift detection due to the assumptions of the normal distribution approximation.
Other algorithms like FLORA use the overall accuracy of the learner as the measure of change, a large decrease in accuracy indicating the occurrence of a concept drift.
\\\\
ADWIN is a popular change detection method based on comparing data distributions from two different windows via the use of an adaptive sliding window. The detection method works by finding two sub-windows of the window W that have significantly different means. When two significantly different sub-windows are found the algorithm indicates a concept drift has occurred, and the window size is decreased by dropping the older sub-window. Otherwise the window size is enlarged when no drift is detected. As the window grows when no shift is detected, there is no upper limit to the window size during long periods of stability ? this leads to an unbounded memory requirement (this problem is remedied in a later version of the algorithm). One limitation of the ADWIN approach is that it may require more memory than sequential or statistical approaches as it has a memory complexity of O(logW) - O(W) compared to constant time O(1) for the other approaches (Gama et al, 2014). However it may give more precise information about the location of the concept drift (Gama et al, 2014). In addition to this, it has strict guarantees on the rate of false positives and false negatives (Goncalves et al). The time complexity of the algorithm is also higher than the other approaches but both experiments by Goncalves and Gama show that the runtime is lower than all other algorithms used in the studies. 
The entropy-based approach also uses sliding windows and is based on a modified version of Shannon's entropy. It uses a sliding 2 window approach, adapting the window size when a shift is detected. When a shift occurs window size is set to the minimum (forgets old instances), and grows with each new instance until the upper limit is reached. 
Although there has been much work done on drift detection techniques there are few studies that attempt to address the class imbalance problem by modification of the drift detection method. Some of the more common approaches for dealing with imbalanced classes are the use of ensembles, and sampling methods such as bagging or boosting.
\\\\
The PerfSim algorithm first trains the learner on a dataset, and produces a confusion matrix that summarises the performance of the learner. After receiving a second dataset (batch of instances) it tests the learner on the new dataset and also produces a summary matrix. The summary statistics are converted into vectors and are compared using a similarity measure (cosine similarity).  This may decrease the effect of class imbalance in comparison to methods that only rely on the overall accuracy where the contributions from the majority classes outweigh the minority classes. This method is independent of the learning algorithm and may be paired with any learner. Antwi et al show that this method does not detect false drifts and performs well when evaluated against other current methods (VC and MMD).
The DDM-OCI method is based on the DDM method proposed by Gama, but specifically attempts to deal with imbalanced classes. This technique uses the recall of the minority class as the measure of change. The warning and drift thresholds are set in a similar manor to DDM and EDDM.

\section{Base learners}
Supervised learning requires models to be trained from labelled data. When model-building, we need an approach that can use explanatory variables and classes in a training set of data to create a set of rules for classifying unseen instances. This approach is our â€˜base learnerâ€™: our tool that will build our classifier to apply to unseen instances of our data \cite{hoe12}.
\\\\
In a streaming environment, however, we can only make one pass through our data. Concept drift requires a learner that can be run at regular intervals, so as to understand the current distribution that our instances fall into. This requires a trade-off between time, accuracy and memory. A successful learner will be sufficiently accurate while running quickly and with manageable memory overhead. These requirements are much more important than in a static environment, where we have a finite amount of data and time to process it.
\\\\
Decision trees have particular features which make them a good fit for classifying streams with concept drift. The C4.5 decision tree, and its predecessor, the ID3 tree, are very popular within the research community \cite{sal94}. Their appeal lies in their simplicity â€“ at every node, a binary decision is made related to one of the explanatory variables, resulting in a â€˜divide-and-conquerâ€™ approach which results in fast classification and low overhead for the model. This leads to some clear drawbacks â€“ only â€˜rectangular regionsâ€™ can be used to discriminate between classes in the feature space for instance. Splits are made through information gain measures, seeking to maximise the change in impurity of the nodes with each split. These trees will naturally overfit, if fully grown, but can be pruned to achieve a desired level of fit to the data. Their adaptability and speed make them a natural fit in classification of streams.
\\\\
Domingos and Hulten \cite{dom00} highlight a major concern with C4.5, ID3 and CART â€“ they assume that all training examples can be held in memory, and are limited in the examples they can consider. Other trees such as SPRINT and SLIQ have attempted to resolve this issue by using a window to scan large datasets sequentially, but this is very slow when building complex trees, and still requires all data to reside in disk-space. They propose the Very Fast Decision Tree (VFDT), which requires that data items are only read once, in a small and constant time. They utilise Hoeffding Bounds \cite{hoe63} to guarantee that the statistically best attribute to divide a node on can very commonly be found using a small number of examples of instances as could be with infinite. They propose the Hoeffding tree, which regularly checks whether splitting a node on the best attribute will lead to an improvement in a selected measure (information gain or Gini) beyond a set threshold. This allows it to continue to adapt, even with infinite data, without sacrificing significant quality nor having burdensome memory requirements. Parameters need to be set by the user: the minimum number of examples before reviewing whether to split a node; the threshold to allow a split; and whether the algorithm can rescan prior examples. Even if these parameters are well selected by the user, there is a danger that they will lead to poor performance in environments featuring significant concept drift.
\\\\
Recognising the issues posed by concept drift, Hulten et al. \cite{hul01} proposed the Concept-Adapting Very Fast Decision Tree (CVFDT), an adaptation on the VFDT to give it the adaptability to handle changing data. Through adopting a sliding window approach, the CVFDT increments counts of recently seen data while reducing counts of older data. This should have no effect when there is no concept drift, but where there is drift, prior splits will no longer pass the Hoeffding tests that deemed them worthwhile. At this point, an alternate subtree is grown by the algorithm from that node. If it accurately classifies new data better than the existing subtree, the existing subtree is replaced. Overall, this requires additional memory to keep alternate subtrees in memory, and summary statistics of split quality at each node. However, these memory requirements are still constant with the data items received. CVFDTs cannot utilise old sub-trees which are discarded, which could provide potential performance improvements where concepts drifts can revert back to prior probability distributions.
\\\\
Decision trees have trouble accurately classifying rare classes. Consider a binary class problem: if a very large majority belongs to one class, often a classifier achieves best performance by classifying all instances as that class. Single classifier often needs to combine in a ensemble to achieve good performance for imbalanced data set, however recent researches has made it possible for some classifiers perform as good as ensemble. Lyon et al \cite{lyo14} propose an improvement on the VFDT by introducing a new criterion for splitting: Hellinger distance. This measure seeks the attributes which are most disjoint in the data, and seeks to create tree splits based on these features. Instead of prioritising information gain, this tree prioritises class discrimination. This will often lead to a smaller portion correctly classified, but should amplify the number of the rare class successfully classified. Calculating Hellinger distance can be costly, as it scales to the number of classes and needs to discretize continuous classes into bins, leading to further multiplicative scaling. Lyon et al propose the HD-VFDT which uses the splitting criterion above, and also the Gaussian Hellinger Very Fast Decision Tree, which assumes normality of the distribution to simplify calculation of the Hellinger Distance. However, instances in a data stream are not independent, as time affects their distribution, and so this could lead to flawed results in some circumstances. Experimental analysis of this approach showed statistically significant improvements using these methods over simple Hoeffding Trees where the minority class makes up 1\% or less of the total data. They do not evaluate the approach thoroughly with regard to time nor memory use. Specifically, it would be interesting to measure the difference in accuracy, speed and time required between the two variants of the Hellinger tree proposed.
\\\\
Liechtenwalter and Chawla \cite{lic10} have found potential in using Hellinger trees in environments with concept drift. By weighting the Hellinger distance with Information Gain to measure distance between datasets, a learner can decide whether model learnt on previous data is valid on current data. Pazzolo et al \cite{paz14} showed, using the HDDT proposed by Cieslak and Chawla \cite{cie08} and C4.5 tree, that trees with this weighting generally outperformed trees without the weighting in class-imbalanced environments with drift. These papers do not address the multi-class problem, which would be a natural extension of this research.

\section{Ensemble learners}

\section{Evaluating approaches}
Both UCI datasets and synthetic datasets are often chosen to evaluate classifiers' performance. In order to form a skewed distribution problem and simulate data stream, a minority class and a majority class have to be chosen from a dataset to form the skewed distribution between two classes, the data then will need to be partitioned into some chunks with skewed distribution. 
\\\\
Efficiency and accuracy are two major factors for evaluating learning algorithms' performance on data stream. In terms of efficiency, it often refers to time overhead, in this sense sampling techniques perform not as efficient as other techniques. In terms of accuracy, there are two measures can be used: (1) probability estimation accuracy (2) classification accuracy. In \cite{gao}, Gao suggested that using Mean Squared Error to measure the quality of probability estimation, for rare class, low Mean Squared Error is desired. Common measurement for classification accuracy is classification error rate, but this measurement is undesirable for imbalanced data streams because the rare minority class doesn't have an significant impact on classification error rate. Instead, three other measurement are typically used: Precision, Recall and False Alarm Rate. Gao suggested ROC curve can show the trade-off between Precision and False Alarm Rate. A good ROC has big Area Under ROC Curve (AUC), and the closer to the left-top corner the better. Another similar method is recall-precision plot. Gao showed that by employing Sampling and Ensemble techniques (SE\cite{se}), it has significantly improved both probability accuracy (MSE) and ROC measurement. In \cite{survey_on_concept_drift_adaptation}, Gama discussed several other performance evaluation metrics can be used:  Sensitivity and Specificity, Kappa-statistic\cite{survey_on_concept_drift_adaptation}. Kappa-statistics is useful for imbalanced dataset. All these performance metrics should be taken look at when considering the basic reference point or baseline.
\\\\
In \cite{survey_on_concept_drift_adaptation}, Gama discussed except above evaluation metrics for learning algorithm, the change detection's accuracy can be evaluated for those who equipped explicit drift detection technique. (1) Probability of true change detection. (2) Probability of false alarms. (3) Delay of detection.
\\\\
In  \cite{survey_on_concept_drift_adaptation}, Gama discussed that traditional cross-validation method is not applicable to data stream because it will not keep the temporal nature. He suggested two procedures instead: (1) Holdout. This is wildly used and most useful method for validation, but it is not always possible, because of the temporal nature of data stream. (2) Interleaved Test-Then-Train or Prequential. This method makes full use of every instance, also has a smooth accuracy plot. (3) Controlled Permutations. This method is useful for sudden drift data stream by randomization.

\section{Discussion}

In this paper thus far, we have outlined four important factors of streaming data analysis that are made more important by the presence of class imbalance and concept drift: sampling; concept drift detection; base learners; and ensemble approach. We have also outlined various ways to evaluate the analysis performance. But how can we decide on a combination that best suits our purpose? Here, we examine research that has tested various approaches of on different measures, and summarise some of the lessons we can learn about designing our own analyses.

Bifet et al. (2009) examines analysis of very large data streams (1-10 mil. Items) with drift present. The authors propose two new methods: bagging Adaptive Size Hoeffding Trees (where multiple Hoeffding Trees of different sizes are bagged, with increased diversity and increased performance resulting); and bagged Hoeffding Trees, with ADWIN as a change detector (removing the worst classifier whenever change is detected). First, they use Kappa statistics to show that the ASHT produces more diverse trees than a standard approach. They then compare their proposed two approaches against decision stumps, Naïve Bayes classifiers, and Hoeffding trees with DDM, EDDM, Hoeffding Option Trees (with and without boosting), bagged HT trees and many others. They measure performance by time, memory and classification accuracy across datasets with various degrees of drift.

Their results demonstrate that there is no global best option. Bagged approaches provide better accuracy than other approaches at the cost of memory and time. However, using 5 trees rather than 10 almost doubles the speed of the approach without sacrificing much accuracy.  The simplest approaches (e.g. Naïve Bayes) generally run in under a tenth of the time of the fastest, sacrificing anywhere between 5\% and 30\% classification accuracy compared to the more complex approaches, dependent on the drift in the underlying dataset. The memory requirements of some approaches such as simple Hoeffding trees stay fairly constant over data items processed, while others such as bagged ASHT continually climb. The paper successfully demonstrates how its two proposed approaches perform very accurate analyses.

This paper shows that before we decide on our approach, we need a clear understanding of the size of the stream, some understanding on whether underlying drift is present, and what constraints (time, memory and required accuracy) we have on our approach. This requires that we research and understand our constraints and data before implementing our approach, as a lot of time can be wasted through implementing insufficient approaches. Without regular (and costly) validation of stream classifiers, we will not fully understand how well our approach is working ? using ensemble methods and including drift-detection helps to provide an approach that is robust in a changing environment.

Dal Pazzolo et al. (2014)  examine imbalanced datasets with varying levels of concept drift, and test various combinations of base learner (Hellinger Distance Decision Tree (HDDT) vs. C4.5), sampling method (baseline, undersampling, and two variations of oversampling the rare class), and ensemble approach (single model vs. an ensemble approach using the weighting of Hellinger Distance and Information Gain described previously). They evaluated their experiments in terms of time and area under the ROC curve.

Their results showed that undersampling and baseline samples performed faster than their oversampling methods. The HDDT trees led to slightly slower performance than the C4.5 trees. The fastest method (undersampling, C4.5 tree) was over twenty times faster than the slowest method (HDIG ensemble, with a HDDT tree and oversampling). AUROC measures were consistently improved by using their ensemble approach and the HDDT tree. The HDDT specifically made a large difference in the dataset with the highest imbalance ratio. However, this improvement was not significant when an ensemble approach was used. Overall, the baseline ensemble HDDT approach and the undersampled ensemble HDDT approach produced significantly better AUROC measures than the other approaches. Most interestingly, there was an interaction between learner type, sampling method, and ensemble approach, where a change in one factor would affect the efficacy of another. This set of results demonstrates two important points. First, without proper understanding of the methods we are using, we can lose time and accuracy through taking a complex approach, such as oversampling with ensemble HDDTs in this experiment. Second, when working with class imbalance, the degree and nature of imbalance will decide the relative improvement a particular approach provides.

Zhang and Soda (2012) examine binary-class classification in skewed data streams. They propose an approach of finding a balance between maximising overall classification accuracy with the balance of accuracy across classes, rather than maximising either. Their approach uses two different ensembles of classifiers (each a combination of Naïve Bayes, logistic regression and C4.5), each with a different method of analysis. The data is divided into batches, or fixed windows. The method used is decided by whether the data is detected to be skewed or not (a measure titled ?reliability?). Each method uses different sampling methods and ensemble voting approaches depending on whether it is for skewed or non-skewed data.

This decision-making process causes the approach to take longer than either individual ensemble, and achieves a lower total accuracy than one measure and class-balanced accuracy than the other. However, it is a strong demonstration of how a combination of methods can result in a system that will intelligently change behaviour based on the data profile, and select a balanced analysis approach. This approach will be robust, even if the user does not have deep understanding of the data nor expert knowledge of the algorithms involved. This shows a potential, new approach to analysing skewed, imbalanced data-streams that could be robust in a variety of situations. This approach doesn?t explicitly address concept drift; once adapted to do so, though, it could provide an extremely robust approach to analysis of unseen data.


\section{Future Work}

Analysis of imbalanced data streams with underlying concept drift is an exciting new area, with much to explore and develop. Here we discuss several areas that have come to our attention that would benefit from additional research.

Many approaches to imbalanced classes are limited to binary classes, and not multi-class problems. For example, Zhang and Soda?s two-ensemble approach uses evaluation measures and learners that cannot function beyond two classes. In the streaming environment, accommodating multiple-classes is expensive and a lot of work will be needed to adapt these approaches.

Performance comparisons between new approaches are few and far between, especially across dimensions explored in this paper. Research that compares performance of existing approaches in total accuracy, class-balanced accuracy, time and memory across different ensemble methods, sampling, learners and concept-drift detection will be valuable.

Finally, selecting an approach right now requires a deep understanding of the data stream. Adaptive approaches that can select powerful approaches to analysing unseen data will be invaluable. These must be acceptably cheap to perform in the worst-case and adaptive enough to adjust to changing underlying data streams.

\section{Conclusion}

In this paper, we have examined analysis of data streams, with a focus on those with class-imbalance and underlying concept drift. We have discussed approaches to sampling, concept drift detection, base learners and ensemble approaches, and how they contribute to analyses. With each of these aspects, we have discussed important and influential techniques that are making an impact within the area. 

We explored measures that can be used to evaluate our methods, and the specific considerations that need to be made with skewed data. Through discussing examples of these approaches put into practice, we have demonstrated the complexity of selecting an approach, and shown how we must understand the constraints and goals of our analysis before deciding on an approach.

Finally, we have proposed several areas of future work that have the potential to make a real contribution to this field.

\newpage
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}